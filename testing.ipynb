{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerax\n",
    "from kerax.layers import Concatenate\n",
    "from kerax.layers import Conv2D, Dense, Flatten, Input, BatchNormalization, Activation, Add\n",
    "from kerax.models import Model\n",
    "from kerax.losses import CategoricalCrossEntropy\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from kerax import activations\n",
    "kerax.enable_jit_execution(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.random.random((128, 28,28,1))\n",
    "train_y = np.random.random((128, 10))\n",
    "\n",
    "val_x = np.random.random((128, 28,28,1))\n",
    "val_y = np.random.random((128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((28, 28, 1))\n",
    "conv1 = Conv2D(64, 3, activation=activations.ReLU)(inputs)\n",
    "conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv1)\n",
    "act1 = Activation('relu')(conv1)\n",
    "conv3 = Conv2D(128, 3, padding='same')(act1)\n",
    "act2 = Activation('relu')(conv3)\n",
    "conv4 = Conv2D(128, 3, padding='same')(act2)\n",
    "add = Add()([conv2, conv4])\n",
    "flatten = Flatten()(add)\n",
    "dense1 = Dense(512, activation='relu')(flatten)\n",
    "dense2 = Dense(10, activation='softmax')(dense1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._validate_init(**kwargs)\n",
    "        self.connected_layers = []\n",
    "        self.connection = namedtuple('Layer', ['layer1', 'layer2'])\n",
    "        self.layers = []\n",
    "        self.connect_layers()\n",
    "\n",
    "    def flatten(self, x):\n",
    "        def _flatten(x, result=[]):\n",
    "            for i in x:\n",
    "                if isinstance(i, list):\n",
    "                    return _flatten(i, result)\n",
    "                else:\n",
    "                    result.append(i)\n",
    "            return result\n",
    "        return _flatten(x, [])\n",
    "\n",
    "    def _validate_init(self, **kwargs):\n",
    "        if (\n",
    "            kwargs.get('inputs', False)\n",
    "            and isinstance(kwargs.get('inputs', False), (list, tuple)) != 1\n",
    "        ):\n",
    "            raise Exception('Use \\'input\\' argument instead of \\'inputs\\' if you want to pass a list or a tuple')\n",
    "        elif (\n",
    "            kwargs.get('input', False)\n",
    "            and isinstance(kwargs.get('input', False), (list, tuple)) >= 1\n",
    "        ):\n",
    "            raise Exception('Use \\'inputs\\' argument instead of \\'input\\' if you want to pass an input layer')\n",
    "\n",
    "        inputs = kwargs.get('inputs', False) or kwargs.get('input', False)\n",
    "\n",
    "        if not inputs:\n",
    "            raise Exception('inputs should be provided')\n",
    "\n",
    "        if isinstance(inputs, (list, tuple)):\n",
    "            self.inputs = self.flatten(inputs)\n",
    "            self.input = None\n",
    "        else:\n",
    "            self.input = inputs\n",
    "            self.inputs = [inputs]\n",
    "\n",
    "        outputs = kwargs.get('outputs', False) or kwargs.get('output', False)\n",
    "\n",
    "        if not outputs:\n",
    "            raise Exception('outputs should be provided')\n",
    "\n",
    "        if (\n",
    "            kwargs.get('outputs', False)\n",
    "            and isinstance(kwargs.get('outputs', False), (list, tuple)) != 1\n",
    "        ):\n",
    "            raise Exception('Use \\'output\\' argument instead of \\'outputs\\' if you want to pass a list or a tuple')\n",
    "        elif (\n",
    "            kwargs.get('output', False)\n",
    "            and isinstance(kwargs.get('output', False), (list, tuple)) >= 1\n",
    "        ):\n",
    "            raise Exception('Use \\'outputs\\' argument instead of \\'output\\' if an output layer')\n",
    "\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            self.outputs = self.flatten(outputs)\n",
    "            self.output = None\n",
    "        else:\n",
    "            self.output = outputs\n",
    "            self.outputs = [outputs]\n",
    "\n",
    "    def get_layers(self):\n",
    "        queue = deque()\n",
    "\n",
    "        if self.inputs:\n",
    "            queue += self.flatten(self.inputs)\n",
    "        else:\n",
    "            queue.append(self.input)\n",
    "\n",
    "        if self.outputs:\n",
    "            queue += self.flatten(self.outputs)\n",
    "        else:\n",
    "            queue.append(self.output)\n",
    "\n",
    "        visited = {i.index for i in queue}\n",
    "        self.layers += queue\n",
    "\n",
    "        while queue:\n",
    "            current_pointer = queue.popleft()\n",
    "            for i in current_pointer.next:\n",
    "\n",
    "                if i.index not in visited:\n",
    "                    self.layers.append(i)\n",
    "                    queue.append(i)\n",
    "                    visited.add(i.index)\n",
    "\n",
    "    def connect_layers(self):\n",
    "        self.get_layers()\n",
    "        self.layers = sorted(self.layers, key=lambda x: x.index)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            self.connected_layers += [self.connection(layer, i) for i in layer.next]\n",
    "    \n",
    "    def check_dependencies(self, layer):\n",
    "        for p in layer.prev:\n",
    "            pass\n",
    "\n",
    "    def have_dependencies(self, layer):\n",
    "        return len(layer.prev) != 0\n",
    "\n",
    "    def same_input_len(self, inputs):\n",
    "        return len(inputs) == len(self.inputs)\n",
    "\n",
    "    def flow_data(self, *args):\n",
    "        if not self.same_input_len(args):\n",
    "            raise Exception(f'Not the same input length expected {len(self.inputs)} found {len(args)}')\n",
    "        \n",
    "        for arg, input_layer in zip(args, self.inputs):\n",
    "            input_layer(arg)\n",
    "\n",
    "        for layers in self.connected_layers:\n",
    "            if layers.layer1.output is not None:\n",
    "                print(layers)\n",
    "                layers.layer2(layers.layer1)\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(input=inputs, output=dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Input Layer>,\n",
       " <Convolutional Layer with input shape (None, 28, 28, 1) and output shape (None, 26, 26, 64)>,\n",
       " <Convolutional Layer with input shape (None, 26, 26, 64) and output shape (None, 26, 26, 128)>,\n",
       " <relu Activation Layer with input shape (None, 26, 26, 64) and output shape (None, 26, 26, 64)>,\n",
       " <Convolutional Layer with input shape (None, 26, 26, 64) and output shape (None, 26, 26, 128)>,\n",
       " <relu Activation Layer with input shape (None, 26, 26, 128) and output shape (None, 26, 26, 128)>,\n",
       " <Convolutional Layer with input shape (None, 26, 26, 128) and output shape (None, 26, 26, 128)>,\n",
       " <Add Layer with input shape (None, 26, 26, 128) and output shape (None, 26, 26, 128)>,\n",
       " <Flatten Layer with input shape (None, 26, 26, 128) and output shape (None, 86528)>,\n",
       " <Dense Layer with input shape (None, 86528) and output shape (None, 512)>,\n",
       " <Dense Layer with input shape (None, 512) and output shape (None, 10)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "inputs(train_x)\n",
    "flag = True\n",
    "for l in g.connected_layers:\n",
    "    if l.layer1.output is not None:\n",
    "        for i in l.layer2.next:\n",
    "            if i.output is None:\n",
    "                flag = False\n",
    "        if flag:\n",
    "            l.layer2(l.layer1.output)\n",
    "        flag = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_1 Conv2D_1\n",
      "Conv2D_1 Conv2D_2\n",
      "Conv2D_1 Activation_1\n",
      "Conv2D_2 Add_1\n",
      "Activation_1 Conv2D_3\n",
      "Conv2D_3 Activation_2\n",
      "Activation_2 Conv2D_4\n",
      "Conv2D_4 Add_1\n",
      "Add_1 Flatten_1\n",
      "Flatten_1 Dense_1\n",
      "Dense_1 Dense_2\n"
     ]
    }
   ],
   "source": [
    "for l in g.connected_layers:\n",
    "    print(l.layer1.name, l.layer2.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not expected shape, input dims should be (None, 28, 28, 1) found (128, 26, 26, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-03ebe7e444fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/kerax/kerax/layers/convolutional.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Not expected shape, input dims should be {self.input_shape} found {inputs.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Not expected shape, input dims should be (None, 28, 28, 1) found (128, 26, 26, 1)"
     ]
    }
   ],
   "source": [
    "inputs(train_x)\n",
    "\n",
    "train_x2 = np.random.random((128, 26,26,1))\n",
    "inputs2(train_x2)\n",
    "for i in g.layers[1:]:\n",
    "    if isinstance(i, Input):\n",
    "        continue\n",
    "    i(i.prev[0].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6721b10ec9b0eeb463be84ab6ef1ed3e9e9b165baec57fa1847c389edb595caf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('jax': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
